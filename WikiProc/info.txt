- Get XML dump from Wikipedia (http://dumps.wikimedia.your.org/backup-index.html)
- Select as much of the text as you want (head -n NUM_LINES wiki_dump.xml > training_text)
- Process the text with WikiExtractor.py (python WikiExtractor.py training_text) (available at: https://github.com/klintan/wikidump-xml-clean/)
- Concat files generated by WikiExtractor (find . -name 'wiki_*' -exec cat {} + > ALL.txt)
- Process the result with format_text.py (python format_text.py processed_text.txt)
- Train the model using the processed text and Word2Vec